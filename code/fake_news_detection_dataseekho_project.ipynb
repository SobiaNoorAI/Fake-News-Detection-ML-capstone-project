{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Problem Statement\n",
    "Predict whether a news is a fake or real based on the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sfc1ya-wsJJn",
    "outputId": "8dc6461b-e4ed-4dfb-b1f6-879992e73a7e"
   },
   "outputs": [],
   "source": [
    "#Install the following packages if you haven't already\n",
    "#!pip install datasets\n",
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "# Download the stopwords dataset\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Load & Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782,
     "referenced_widgets": [
      "c69ce58322134e3da333748e299bc2df",
      "35b66a1a15a1464faead47eecd05c635",
      "03a31974b99c446680fe620d08e9995c",
      "833ef2bd6d184b2298e08931db56f962",
      "da666a0f55004fe48e2d4f2f009a41d4",
      "96271ad7a411474c9112f3337a1a9ece",
      "bda3d57bf2df4f2f8dd2fc1f2fa2ca8c",
      "0c9afc2ef06a42afb4aea3e25c1087eb",
      "ab60d008116240c59d87e1f13f212b8f",
      "ccf472e769864f1bb1f6d82a6b35d3a2",
      "afd58c479131490c8b1313b866af03bf",
      "d684558a31ff4809a6ffa4d7d1a2948d",
      "cc55389e19354fdbb9f875077fc83a36",
      "e210456af4e1472a96b33ac7ea1dd687",
      "4262649672544ca28d3eabdcc60a2a08",
      "bfadb98ad24b4e86af722576a9bfd7b3",
      "643b8fd8bd1e4bdbb26ac52fc722ef52",
      "cb4b209355604a0e921fc10b8516f94c",
      "65b8372eef4e4fb491afe9febf84cdd5",
      "9dd0cddb253f41a684f29c1ea2e12546",
      "329c44f018ae4476991a8f40c7088c5b",
      "a75daf0df56141479d1a62494871d2cf",
      "ae32b7dfc17d4b6cba16e2e5e2f964a4",
      "852a3fca00f245ebba78b93fbbb5a2e5",
      "1d9525a1db2a40b18c43b5b50c8dffa5",
      "5b6833cf8bc5454a8905858cab55e3a6",
      "6d90f05ddeb0465bb0ec4d98c34b70bd",
      "70d9bd8bdc154add908400ef7436d36d",
      "04da7fbed14942dcb30c4a13db22b6db",
      "a4345f336ff248f28137d217917adf57",
      "27ef18c63fcc478fab219a069badf7f3",
      "3543187bd4604fceb4db8cb03350005f",
      "c1ad6b2fc5f74b5b85442c76763e669f",
      "7c184f6c08fc47fcbdad8b86c0b717f2",
      "8eb892e27ea94c168da99a3c496f6096",
      "199c4101068f4007aa88189333e28cb9",
      "6f2615734e5846079d61adb69c13d8b9",
      "006728763b044529a8d1e6b150d4e65b",
      "bc87369026aa4deab82ddf4be8e77209",
      "b3ad223b887144b09cf560e11c515f1e",
      "23763433bbd24f7a96107b3756156aed",
      "b4b7b3b3854f439b9cb835c2e3fed980",
      "be424292489a4a2eaedb10f9769288b6",
      "c1252a48faef48649a26bfa0b0cd02ef",
      "c495347cb88e459e8c0296e98862199c",
      "c9c314a728484d418ebf6343da7819e8",
      "46efb3de97fb4a2e9720bce31981de9a",
      "881001022c4a4684a6a135256dd722ac",
      "e5a275fdeb5b426f8a21d059b6519080",
      "cb57cb978d0442dcb3f6aa3676afa5a3",
      "b34058a7080b41c58e6dbf0654c59dbf",
      "78f31d0859124a43b5fd78d8ee6dd5b2",
      "2e7745f82fe14e9384bd78f3b49e205e",
      "d93861d87ef0480a9728543fa8e0061b",
      "91a5b3a4fff14c8caeac695da4d07f0c",
      "8223d9dc5cac4070aa6920e4af47a7c7",
      "706fd29ee2724f32acdc58989bc37850",
      "bcbbe42d54a044dc9c3206d9fc2b817a",
      "6b64d28a624c4f64955d6fcd8a3f31f9",
      "05b247588ce3445ea0584cc3e4aa274b",
      "b8f5a57244fd44db9d6005002f93a6e2",
      "f501322e1b634fbaa45be98f8af25fbb",
      "d167d8fdedbd4aa19ffabd0942f34f70",
      "c1cb78cf2cb34678bf03e76b917e9aed",
      "ca3d65c805b74278a4ff93a0fdb51bb3",
      "550d90daa4a44856b1d13ff8ab3ad255",
      "e17ae632e0ca4f43ba4812bbac5c724f",
      "17d8459813c744dfb1919c3309a0fd7f",
      "f815050317e349c9ae06fcbecfb83926",
      "38b1815a548a4b23a61cf9bb3840939e",
      "06713f8404f44e839284e52edc7a0bb3",
      "61442f9b8b4c44fc85b76ec900a22b84",
      "80bf4afd5ced42a0a31c31677958baa9",
      "30676f0fa0f6472f85a782b840a72848",
      "6c9b59da54fe4b8e9e10c7abbd975cee",
      "67eb81ef79214e5ea0f2d4e830d71096",
      "8633d954e7a841ff8c12bbf907fbad9c"
     ]
    },
    "id": "vq3ahUitDSr7",
    "outputId": "2a98cde6-a2c9-40ec-f327-f3464c634ba6"
   },
   "outputs": [],
   "source": [
    "# Directly load the dataset\n",
    "dataset = load_dataset(\"ErfanMoosaviMonazzah/fake-news-detection-dataset-English\")\n",
    "\n",
    "# Access the training data\n",
    "train_data = dataset['train']\n",
    "print(train_data)\n",
    "\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t52z1UAot5ZF"
   },
   "outputs": [],
   "source": [
    "# # Directly load the dataset\n",
    "# dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n",
    "\n",
    "# # Access the training data\n",
    "# train_data = dataset['train']\n",
    "\n",
    "# # Convert to pandas DataFrame\n",
    "# df = pd.DataFrame(train_data)\n",
    "\n",
    "# # Display the first few rows of the dataset\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "l-tZ0j5gIMBU",
    "outputId": "88533314-a3f8-418a-dfe1-5dbce269c89c"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting unwanted columns that are not needed for the analysis\n",
    "clean_df = df.drop(\"Unnamed: 0\", axis = 1)\n",
    "#clean_df = df.drop(\"title\", axis = 1)\n",
    "#clean_df = df.drop(\"subject\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-2tLjveuGkb",
    "outputId": "a39778ee-5d2d-4bba-a558-80b999a1a0cd"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\\n\", clean_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "clean_df = clean_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle Rows Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "nHUDRXEbyzuR",
    "outputId": "ff59911c-0686-4a9b-c094-158ee3ef4dc7"
   },
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=clean_df, x=\"label\", palette=\"viridis\", hue=\"label\", legend=False)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Label\")  # Optional: label for x-axis\n",
    "plt.ylabel(\"Count\")  # Optional: label for y-axis\n",
    "plt.savefig(\"../images/class_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting The Number Of Samples In 'subject'\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.countplot(y=\"subject\", data=clean_df, palette=\"viridis\", hue=\"subject\", legend=False)\n",
    "plt.title(\"Number of Samples in Subject\")\n",
    "plt.savefig(\"../images/subject_count.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins for the histogram\n",
    "bins = np.linspace(0, 300, 40)\n",
    "\n",
    "# Plot overlapping histograms for label 0 and label 1\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(clean_df[clean_df[\"label\"] == 0][\"text\"].str.len(), bins, alpha=0.5, label=\"Label 0 (Real)\", color=\"green\")\n",
    "plt.hist(clean_df[clean_df[\"label\"] == 1][\"text\"].str.len(), bins, alpha=0.5, label=\"Label 1 (Fake)\", color=\"red\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Overlap Histogram of Text Length by Label\")\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"../images/overlap_histogram.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove stopwords & convert it lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "uxP2vML2XWna",
    "outputId": "4d732898-8f5d-4625-a8f5-64faa25c096a"
   },
   "outputs": [],
   "source": [
    "# Define the cleaning function\n",
    "def data_cleaning(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove non-alphabet characters, URLs, and extra spaces\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub('(^\\s+|\\s+$)', ' ', text)\n",
    "    text = re.sub(\"@[\\w\\d]+\", ' ', text)  # Remove @ mentions\n",
    "    text = re.sub(\"http:[\\w\\:\\/\\.]+\", ' ', text)  # Remove URLs\n",
    "\n",
    "    # Convert to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply lemmatization\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply data cleaning function to the DataFrame\n",
    "clean_df[\"cleaned_text\"] = clean_df[\"text\"].apply(data_cleaning)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "zuPG80sby1b3",
    "outputId": "8d974f08-1e53-4d28-dc8a-d17a7dd7b50f"
   },
   "outputs": [],
   "source": [
    "# Check text length distribution\n",
    "clean_df['text_length'] = clean_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(clean_df['text_length'], bins=30, kde=True)\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.savefig(\"text_length_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of Top Common words in Original and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "Zxz_d2zFYlmc",
    "outputId": "9362f83f-330c-445e-8113-2963d243798b"
   },
   "outputs": [],
   "source": [
    "# Tokenize all text in the 'text' column\n",
    "all_words_text = \" \".join( clean_df[\"text\"]).split()\n",
    "\n",
    "# Tokenize all text in the 'cleaned_text' column\n",
    "all_words_cleaned = \" \".join(clean_df[\"cleaned_text\"]).split()\n",
    "\n",
    "# Count word frequency for both columns\n",
    "word_freq_text = Counter(all_words_text)\n",
    "word_freq_cleaned = Counter(all_words_cleaned)\n",
    "\n",
    "# Get the most common 20 words for both\n",
    "common_words_text = word_freq_text.most_common(20)\n",
    "common_words_cleaned = word_freq_cleaned.most_common(20)\n",
    "\n",
    "# Convert both to DataFrames\n",
    "common_df_text = pd.DataFrame(common_words_text, columns=[\"Word\", \"Frequency\"])\n",
    "common_df_cleaned = pd.DataFrame(common_words_cleaned, columns=[\"Word\", \"Frequency\"])\n",
    "\n",
    "# Create side-by-side subplots\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot most common words for 'text' column in the first subplot\n",
    "plt.subplot(1, 2, 1)  # (rows, cols, position)\n",
    "sns.barplot(x=\"Word\", y=\"Frequency\", data=common_df_text, palette=\"viridis\", hue=\"Word\", legend=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Most Frequent Words in Original Text\")\n",
    "\n",
    "# Plot most common words for 'cleaned_text' column in the second subplot\n",
    "plt.subplot(1, 2, 2)  # (rows, cols, position)\n",
    "sns.barplot(x=\"Word\", y=\"Frequency\", data=common_df_cleaned, palette=\"viridis\", hue=\"Word\", legend=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Most Frequent Words in Cleaned Text\")\n",
    "\n",
    "# Display both plots side by side\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.savefig(\"../images/word_frequency.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of Top Bigrams in Original and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "l72P7Uh9zC5z",
    "outputId": "2b733a65-51a0-4a52-94cc-617f2c37bcb2"
   },
   "outputs": [],
   "source": [
    "# Function to extract top n-grams\n",
    "def get_top_ngrams(text_series, n, top_n=20):\n",
    "    all_words = \" \".join(text_series).split()\n",
    "    ngrams_list = list(ngrams(all_words, n))\n",
    "    ngram_freq = Counter(ngrams_list)\n",
    "    return ngram_freq.most_common(top_n)\n",
    "\n",
    "# Get top bigrams for 'text' column\n",
    "bigrams_text = get_top_ngrams(clean_df[\"text\"], 2)\n",
    "\n",
    "# Get top bigrams for 'cleaned_text' column\n",
    "bigrams_cleaned = get_top_ngrams(clean_df[\"cleaned_text\"], 2)\n",
    "\n",
    "# Convert both to DataFrames\n",
    "bigram_df_text = pd.DataFrame(bigrams_text, columns=[\"Bigram\", \"Frequency\"])\n",
    "bigram_df_cleaned = pd.DataFrame(bigrams_cleaned, columns=[\"Bigram\", \"Frequency\"])\n",
    "\n",
    "# Create side-by-side subplots\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot bigrams for 'text' column in the first subplot\n",
    "plt.subplot(1, 2, 1)  # (rows, cols, position)\n",
    "sns.barplot(x=[\" \".join(b) for b in bigram_df_text[\"Bigram\"]], y=bigram_df_text[\"Frequency\"], palette=\"coolwarm\", hue=[\" \".join(b) for b in bigram_df_text[\"Bigram\"]], legend=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Most Common Bigrams in Original Text\")\n",
    "\n",
    "# Plot bigrams for 'cleaned_text' column in the second subplot\n",
    "plt.subplot(1, 2, 2)  # (rows, cols, position)\n",
    "sns.barplot(x=[\" \".join(b) for b in bigram_df_cleaned[\"Bigram\"]], y=bigram_df_cleaned[\"Frequency\"], palette=\"coolwarm\", hue=[\" \".join(b) for b in bigram_df_cleaned[\"Bigram\"]], legend=False)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Most Common Bigrams in Cleaned Text\")\n",
    "\n",
    "# Display both plots side by side\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.savefig(\"../images/bigrams.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Words in Original and Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "MMc5p9IXzN39",
    "outputId": "8ba2fbfb-03a5-49aa-d30f-c637292b76b2"
   },
   "outputs": [],
   "source": [
    "# Get unique labels\n",
    "unique_labels = clean_df[\"label\"].unique()\n",
    "n_labels = len(unique_labels)\n",
    "\n",
    "# Create figure with appropriate size (2 columns, n_labels rows)\n",
    "plt.figure(figsize=(15, 5*n_labels))\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    # First subplot: Word cloud for 'text' column\n",
    "    plt.subplot(n_labels, 2, i*2+1)\n",
    "    words_text = \" \".join(clean_df[clean_df[\"label\"] == label][\"text\"])\n",
    "    wordcloud_text = WordCloud(width=250, height=150, background_color=\"black\").generate(words_text)\n",
    "    plt.imshow(wordcloud_text, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud - Sentiment {label} (Raw Text)\")\n",
    "\n",
    "    # Second subplot: Word cloud for 'cleaned_text' column\n",
    "    plt.subplot(n_labels, 2, i*2+2)\n",
    "    words_cleaned = \" \".join(clean_df[clean_df[\"label\"] == label][\"cleaned_text\"])\n",
    "    wordcloud_cleaned = WordCloud(width=250, height=150, background_color=\"white\").generate(words_cleaned)\n",
    "    plt.imshow(wordcloud_cleaned, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud - Sentiment {label} (Cleaned Text)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Graph Ideas: histogram of fake , real news\n",
    "\n",
    "\n",
    "bae plot of text, title, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "Assigns weights to words based on importance in the document and corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(clean_df[\"cleaned_text\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display TF-IDF features\n",
    "print(X_tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Input, Output Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X (features) and y (labels)\n",
    "X = X_tfidf # Cleaned_text \n",
    "y = clean_df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80-20 ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Navive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_preds = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Logistics Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_preds = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate both models with a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HV9CfnteKl1p",
    "outputId": "71b50aff-9f8d-4f21-90b7-7b573b635e7d"
   },
   "outputs": [],
   "source": [
    "# Define a function for evaluation\n",
    "def evaluate_model(model_name, y_test, y_pred):\n",
    "    print(f\"\\n🔹 Model: {model_name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Evaluate models\n",
    "evaluate_model(\"Naive Bayes\", y_test, nb_preds)\n",
    "evaluate_model(\"Logistic Regression\", y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Logistic Regression\n",
    "sns.heatmap(confusion_matrix(y_test, lr_preds), annot=True, fmt=\"d\", cmap=\"Reds\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.savefig(\"../images/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Model\n",
    "xgb = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"Accuracy: {xgb_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "svm_preds = svm.predict(X_test)\n",
    "print(\"Support Vector Machine Report:\")\n",
    "print(classification_report(y_test, svm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "print(\"Random Forest Report:\")\n",
    "print(classification_report(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names and their respective accuracies in percentage\n",
    "model_names = [\"Naive Bayes\", \"Logistic Regression\", \"XGBoost\", \"SVM\", \"Random Forest\"]\n",
    "accuracies_percentage = [\n",
    "    accuracy_score(y_test, nb_preds) * 100,\n",
    "    accuracy_score(y_test, lr_preds) * 100,\n",
    "    xgb_accuracy * 100,\n",
    "    accuracy_score(y_test, svm_preds) * 100,\n",
    "    accuracy_score(y_test, rf_preds) * 100\n",
    "]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = sns.barplot(x=model_names, y=accuracies_percentage, palette=\"viridis\")\n",
    "\n",
    "# Add text labels with each bar's value\n",
    "for bar, accuracy in zip(bars.patches, accuracies_percentage):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,  # X-coordinate\n",
    "        #bar.get_height() + 1,              # Y-coordinate (slightly above the bar)\n",
    "        bar.get_height()-50,              # Y-coordinate (slightly above the bar)\n",
    "        f\"{accuracy:.2f}%\",                # Text label\n",
    "        ha=\"center\", va=\"bottom\", fontsize=12, color=\"white\"\n",
    "    )\n",
    "\n",
    "# Customize the chart\n",
    "plt.title(\"Model Accuracy Comparison (in Percentage)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim(0, 100)  # Set y-axis range to [0, 100]\n",
    "plt.savefig(\"../images/model_accuracy_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Prediction on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Dm0yR1z0nmh",
    "outputId": "ab0d7b5f-fab6-48da-c658-7b606df84071"
   },
   "outputs": [],
   "source": [
    "#def predict_sentiment(text, model):\n",
    " #   cleaned_text = data_cleaning(text)\n",
    "  #  print(cleaned_text)\n",
    "   # text_vectorized = tfidf_vectorizer.transform([cleaned_text])\n",
    "    #prediction = model.predict(text_vectorized)\n",
    "    #return prediction[0]\n",
    "\n",
    "# Test the model with a sample text\n",
    "#sample_text = \"Trump is working hard for US.\"\n",
    "#print(\"Predicted Sentiment:\", predict_sentiment(sample_text, lr_model))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
